{
	random_seed:{
		tensorflow: 3498734
		numpy: 89348932
	}
	logging:{
		frequency: 100
		path: 'data/celeba_1024_haar/logs/'
	}
	checkpoints:{
		frequency: 20000
		path: 'data/celeba_1024_haar/checkpoints/'
	}
	model:{
		n_levels: 10
		base_level: 0
		spatial_biasing: [false,false,false,false,false,false,false,false,false,false,false]
		steps_per_resolution: [8,8,16,16,16,16,16,16,16,16,16]
		conv_widths: [64,64,64,128,128,128,128,128,128,128,128]
		n_res_blocks: 3
		actnorm:{
			logscale: 3.0
		}
		invertible_1x1:{
			use_lu: false # currently unused
		}
		openai_conv:{
			zero_use_logscale: true
			zero_logscale_factor: 3.0
			normalized: true
		}
		data:{
			n_bits: 8
			dimensions:{
				h: 1024
				w: 1024
				c: 3
			}
		}
	}
	training:{
		partial_training_crops: [1,1,1,1,1,1,2,2,4,8,16]
		n_batch: [64,64,64,64,64,64,64,64,64,64,64]
		n_ddi_batch: [64,64,64,64,64,64,64,64,64,64,64]
		optimizer: 'adamax'
		base_learning_rate: 0.001
		iterations: 5000000
		ramp_up_iterations: 125000
		data:{
			root_path: 'dataset_dummy/'
			path: 'datalists/dummy_train_1024.txt'
		}
	}
	validation:{
		partial_training_crops: [1,1,1,1,1,1,1,1,1,1,1]
		frequency: 2000
		n_batch: [128,128,128,128,128,128,128,128,128,32,16]
		visualizations: true
		data:{
			root_path: 'dataset_dummy/'
			path: 'datalists/dummy_val_1024.txt'
		}
	}
	debug:{
		check_numerics: false
	}
	optimizer:{
		adamax:{
			beta1: 0.9,
			weight_decay: 1.0,
			polyak_epochs: 1.0, # set to constant, not much tuning was done with this
			train_its: 782 # set to constant, not much tuning was done with this
		}
	}
	offline_sampling:{
		sample_path: 'data/celeba_1024_haar/samples/'
	}
}
